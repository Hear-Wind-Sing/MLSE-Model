{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc9f8f-2978-45c9-9eb3-9d4b3684c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy mlxtend lightgbm hyperopt shap matplotlib seaborn lime pdpbox pyALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f175c-cd1d-4b4b-ada4-62abb0504d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor as RFR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "import warnings\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, explained_variance_score\n",
    "import shap\n",
    "from sklearn.linear_model import HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1ed30-8fe3-452a-bb10-c1faa2af4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_excel(r'C:\\Users\\28901\\Desktop\\水生生物\\新的分子描述符\\17个分子描述符鱼类带Y.xlsx', index_col=0)\n",
    "\n",
    "X = data.iloc[:, 1:-1]  # 特征列\n",
    "Y = data.iloc[:, -1]  # 目标列\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=100)\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "Xtrain_scaled = scaler_X.fit_transform(Xtrain)\n",
    "Xtest_scaled = scaler_X.transform(Xtest)\n",
    "\n",
    "clf1 = LinearRegression()\n",
    "clf2 = Ridge(alpha=1.0)\n",
    "clf3 = SVR(C=1.0, epsilon=0.2)\n",
    "clf4 = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.05, num_leaves=31) \n",
    "clf5 = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=100)\n",
    "\n",
    "meta_regressor = RFR(n_estimators=100, max_depth=5, random_state=100)\n",
    "\n",
    "def hyperopt_objective_LGB(params):\n",
    "    clf4 = lgb.LGBMRegressor(\n",
    "        n_estimators=int(params[\"n_estimators\"]),\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        num_leaves=int(params[\"num_leaves\"])\n",
    "    )\n",
    "    stack = StackingRegressor(estimators=[('clf1', clf1), ('clf2', clf2), ('clf3', clf3), \n",
    "                                          ('clf4', clf4), ('clf5', clf5)], final_estimator=meta_regressor)\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "    validation_loss = cross_validate(stack, Xtrain_scaled, Ytrain, scoring=\"r2\", cv=cv, n_jobs=-1)\n",
    "    return -np.mean(validation_loss[\"test_score\"])\n",
    "\n",
    "def hyperopt_objective_MLP(params):\n",
    "    clf5 = MLPRegressor(\n",
    "        hidden_layer_sizes=(int(params[\"hidden_layer_size\"]),),\n",
    "        max_iter=1000,\n",
    "        random_state=100\n",
    "    )\n",
    "    stack = StackingRegressor(estimators=[('clf1', clf1), ('clf2', clf2), ('clf3', clf3), \n",
    "                                          ('clf4', clf4), ('clf5', clf5)], final_estimator=meta_regressor)\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "    validation_loss = cross_validate(stack, Xtrain_scaled, Ytrain, scoring=\"r2\", cv=cv, n_jobs=-1)\n",
    "    return -np.mean(validation_loss[\"test_score\"])\n",
    "\n",
    "def hyperopt_objective_Ridge(params):\n",
    "    clf2 = Ridge(alpha=params[\"alpha\"])\n",
    "    stack = StackingRegressor(estimators=[('clf1', clf1), ('clf2', clf2), ('clf3', clf3), \n",
    "                                          ('clf4', clf4), ('clf5', clf5)], final_estimator=meta_regressor)\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "    validation_loss = cross_validate(stack, Xtrain_scaled, Ytrain, scoring=\"r2\", cv=cv, n_jobs=-1)\n",
    "    return -np.mean(validation_loss[\"test_score\"])\n",
    "\n",
    "def hyperopt_objective_SVR(params):\n",
    "    clf3 = SVR(C=params[\"C\"], epsilon=params[\"epsilon\"])\n",
    "    stack = StackingRegressor(estimators=[('clf1', clf1), ('clf2', clf2), ('clf3', clf3), \n",
    "                                          ('clf4', clf4), ('clf5', clf5)], final_estimator=meta_regressor)\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "    validation_loss = cross_validate(stack, Xtrain_scaled, Ytrain, scoring=\"r2\", cv=cv, n_jobs=-1)\n",
    "    return -np.mean(validation_loss[\"test_score\"])\n",
    "\n",
    "def hyperopt_objective_RFR(params):\n",
    "    meta_regressor = RFR(n_estimators=int(params[\"n_estimators\"]), \n",
    "                         max_depth=int(params[\"max_depth\"]), \n",
    "                         min_samples_split=int(params[\"min_samples_split\"]),\n",
    "                         random_state=100)\n",
    "    \n",
    "    stack = StackingRegressor(estimators=[('clf1', clf1), ('clf2', clf2), ('clf3', clf3), \n",
    "                                          ('clf4', clf4), ('clf5', clf5)], final_estimator=meta_regressor)\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "    validation_loss = cross_validate(stack, Xtrain_scaled, Ytrain, scoring=\"r2\", cv=cv, n_jobs=-1)\n",
    "    return -np.mean(validation_loss[\"test_score\"])\n",
    "    \n",
    "param_space_LGB = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 10),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 100, 10),\n",
    "}\n",
    "\n",
    "param_space_MLP = {\n",
    "    'hidden_layer_size': hp.quniform('hidden_layer_size', 50, 200, 10),\n",
    "}\n",
    "\n",
    "param_space_Ridge = {\n",
    "    'alpha': hp.uniform('alpha', 0.1, 10),\n",
    "}\n",
    "\n",
    "param_space_SVR = {\n",
    "    'C': hp.uniform('C', 0.1, 10),\n",
    "    'epsilon': hp.uniform('epsilon', 0.01, 0.1),\n",
    "}\n",
    "\n",
    "param_space_RFR = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 10),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n",
    "    'min_samples_split': hp.quniform('min_samples_split', 2, 10, 1),\n",
    "}\n",
    "\n",
    "def optimize_hyperparameters_LGB(param_space):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=hyperopt_objective_LGB, space=param_space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "    print(f\"Best Parameters for LGB:\", best_params)\n",
    "    return best_params\n",
    "\n",
    "def optimize_hyperparameters_MLP(param_space):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=hyperopt_objective_MLP, space=param_space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "    print(f\"Best Parameters for MLP:\", best_params)\n",
    "    return best_params\n",
    "\n",
    "def optimize_hyperparameters_Ridge(param_space):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=hyperopt_objective_Ridge, space=param_space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "    print(f\"Best Parameters for Ridge:\", best_params)\n",
    "    return best_params\n",
    "\n",
    "def optimize_hyperparameters_SVR(param_space):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=hyperopt_objective_SVR, space=param_space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "    print(f\"Best Parameters for SVR:\", best_params)\n",
    "    return best_params\n",
    "\n",
    "def optimize_hyperparameters_RFR(param_space):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=hyperopt_objective_RFR, space=param_space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "    print(f\"Best Parameters for RFR:\", best_params)\n",
    "    return best_params\n",
    "\n",
    "best_params_LGB = optimize_hyperparameters_LGB(param_space_LGB)\n",
    "best_params_MLP = optimize_hyperparameters_MLP(param_space_MLP)\n",
    "best_params_Ridge = optimize_hyperparameters_Ridge(param_space_Ridge)\n",
    "best_params_SVR = optimize_hyperparameters_SVR(param_space_SVR)\n",
    "best_params_RFR = optimize_hyperparameters_RFR(param_space_RFR)\n",
    "\n",
    "optimized_clf4 = lgb.LGBMRegressor(\n",
    "    n_estimators=int(best_params_LGB['n_estimators']),\n",
    "    learning_rate=best_params_LGB['learning_rate'],\n",
    "    num_leaves=int(best_params_LGB['num_leaves'])\n",
    ")\n",
    "\n",
    "optimized_clf5 = MLPRegressor(\n",
    "    hidden_layer_sizes=(int(best_params_MLP['hidden_layer_size']),),\n",
    "    max_iter=1000,\n",
    "    random_state=100\n",
    ")\n",
    "\n",
    "optimized_clf2 = Ridge(alpha=best_params_Ridge['alpha'])\n",
    "\n",
    "optimized_clf3 = SVR(C=best_params_SVR['C'], epsilon=best_params_SVR['epsilon'])\n",
    "\n",
    "meta_regressor = RFR(n_estimators=int(best_params_RFR['n_estimators']),\n",
    "                     max_depth=int(best_params_RFR['max_depth']),\n",
    "                     min_samples_split=int(best_params_RFR['min_samples_split']),\n",
    "                     random_state=100)\n",
    "\n",
    "stack_optimized = StackingRegressor(\n",
    "    estimators=[('clf1', clf1), ('clf2', optimized_clf2), ('clf3', optimized_clf3), \n",
    "                ('clf4', optimized_clf4), ('clf5', optimized_clf5)],\n",
    "    final_estimator=meta_regressor\n",
    ")\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "validation_loss = cross_validate(stack_optimized, Xtrain_scaled, Ytrain, scoring=\"r2\", cv=cv, n_jobs=-1)\n",
    "score_cv = np.mean(validation_loss[\"test_score\"])\n",
    "\n",
    "stack_optimized.fit(Xtrain_scaled, Ytrain)\n",
    "\n",
    "y_pred = stack_optimized.predict(Xtest_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fd17a-559b-4e13-bb09-6884e6c4a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, delta=1.35):\n",
    "    error = y_true - y_pred\n",
    "    loss = np.where(\n",
    "        np.abs(error) <= delta,\n",
    "        0.5 * error ** 2,  # 对于小误差部分\n",
    "        delta * (np.abs(error) - 0.5 * delta)  # 对于大误差部分\n",
    "    )\n",
    "    return np.mean(loss)\n",
    "\n",
    "huber_loss_value = huber_loss(Ytest, y_pred)\n",
    "\n",
    "print(\"Huber loss:\", huber_loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd6e4f-60d3-4e4f-b2e1-3c20ed12b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(Ytest, y_pred)\n",
    "rmse_score = np.sqrt(mse_score)\n",
    "print(f'堆叠回归器 R²（测试集）：{r2}')\n",
    "print(f'堆叠回归器 RMSE：{rmse_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0d54d-5525-49b3-b54f-a4f2e38f03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_test = r2_score(Ytest, y_pred)\n",
    "\n",
    "z = np.polyfit(Ytest, y_pred, 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "x_min, x_max = np.min(Ytest) - 0.5, np.max(Ytest) + 0.5\n",
    "x_extended = np.linspace(x_min, x_max, 500) \n",
    "y_fit_extended = p(x_extended)\n",
    "\n",
    "scale_factor = 1.5 \n",
    "center_x = (x_max + x_min) / 2 \n",
    "relative_width = np.abs(x_extended - center_x) / (x_max - x_min) \n",
    "std_residuals = np.std(y_pred - p(Ytest)) \n",
    "ci_upper = y_fit_extended + scale_factor * std_residuals * (1 + relative_width)  \n",
    "ci_lower = y_fit_extended - scale_factor * std_residuals * (1 + relative_width) \n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=1200)\n",
    "\n",
    "plt.scatter(Ytest, y_pred, color='coral', label=\"Predicted values\", alpha=0.6)\n",
    "\n",
    "plt.plot(x_extended, x_extended, color='grey', linestyle='--', label=\"1:1 Line\", linewidth=1.5)\n",
    "\n",
    "plt.plot(x_extended, y_fit_extended, color='blue', label=f\"Line of Best Fit\", linewidth=2)\n",
    "\n",
    "plt.fill_between(x_extended, ci_lower, ci_upper, color='blue', alpha=0.2, label=\"95% Confidence Interval\")\n",
    "\n",
    "plt.title(\"Model 3\", fontsize=14, fontname=\"Times New Roman\")\n",
    "plt.xlabel(\"Actual Values\", fontsize=12, fontname=\"Times New Roman\")\n",
    "plt.ylabel(\"Predicted values\", fontsize=12, fontname=\"Times New Roman\")\n",
    "\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_linewidth(1.5)\n",
    "ax.spines['right'].set_linewidth(1.5)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.tick_params(axis='x', which='both', direction='out', width=1.5)\n",
    "ax.tick_params(axis='y', which='both', direction='out', width=1.5)\n",
    "\n",
    "plt.savefig(r'C:\\Users\\28901\\Desktop\\水生生物\\图\\鱼类毒性拟合图new1.24.png', format='png',dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc1d1a6-0f3a-4601-82ed-a5d158445984",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_excel(r'C:\\Users\\1\\Desktop\\水生生物\\新的分子描述符\\51个分子描述符预测.xlsx', index_col=0)\n",
    "X_new = new_data.iloc[:, 1:]\n",
    "X_new_scaled = scaler_X.transform(X_new)\n",
    "y_new_pred = stack_optimized.predict(X_new_scaled)\n",
    "new_data['Predicted_Y'] = y_new_pred\n",
    "new_data.to_excel(r'C:\\Users\\1\\Desktop\\水生生物\\新的分子描述符\\51个分子描述符预测鱼类毒性12.20.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e032842-cc87-4c4e-8532-fa849f6c8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(stack_optimized, Xtest_scaled, Ytest, n_repeats=10, random_state=42)\n",
    "importance = result.importances_mean\n",
    "sorted_idx = importance.argsort()[::-1] \n",
    "sorted_importance = importance[sorted_idx] \n",
    "sorted_features = X.columns[sorted_idx] \n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.figure(figsize=(10, 8)) \n",
    "sns.barplot(x=sorted_importance, y=sorted_features, color=\"#24d7d0\")\n",
    "plt.title(r'$\\it{Danio\\ rerio}$ toxicity Feature Importance', fontweight='bold', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
    "plt.xlim(0, max(sorted_importance) * 1.1) \n",
    "plt.gca().invert_yaxis() \n",
    "plt.tight_layout() \n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_linewidth(2) \n",
    "save_path = r'C:\\Users\\1\\Desktop\\水生生物\\shap\\特征重要性鱼类毒性.png' \n",
    "plt.savefig(save_path, format='png', dpi=600) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70513a7b-f98d-4a61-a0a8-95f902d3357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "features = ['Chi1', 'NumRotatableBonds', 'HeavyAtomCount']\n",
    "for feature in features:\n",
    "    feature_index = X.columns.get_loc(feature)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  \n",
    "    disp = PartialDependenceDisplay.from_estimator(\n",
    "        stack_optimized,             \n",
    "        Xtrain_scaled,             \n",
    "        features=[feature_index],   \n",
    "        feature_names=X.columns,    \n",
    "        grid_resolution=50,         \n",
    "        ax=ax                     \n",
    "    )\n",
    "    for ax in disp.axes_.ravel():\n",
    "        ax.set_title('')  \n",
    "    for i, ax in enumerate(disp.axes_.ravel()):\n",
    "        ax.set_xlabel(feature, fontsize=12, fontweight='bold')\n",
    "\n",
    "    for ax in disp.axes_.ravel():\n",
    "        for line in ax.get_lines():\n",
    "            line.set_color(\"#9900FA\") \n",
    "        for collection in ax.collections: \n",
    "            collection.set_facecolor(\"#9900FA\")\n",
    "    for ax in disp.axes_.ravel():\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(2) \n",
    "    plt.subplots_adjust(hspace=1.0, top=0.95)\n",
    "    save_path = f'C:/Users/1/Desktop/水生生物/依赖图/鱼类/{feature}_PDP.png' \n",
    "    plt.savefig(save_path, format='png', dpi=600, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Partial dependence plot for {feature} saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee660ad-3bfb-47f1-a271-c89f48dbc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import os\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "background = np.vstack([Xtrain_scaled, Xtest_scaled])\n",
    "explainer = shap.KernelExplainer(stack_optimized.predict, background)\n",
    "shap_values = explainer.shap_values(Xtest_scaled)\n",
    "save_dir = r'C:\\Users\\1\\Desktop\\水生生物\\shap'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "summary_plot_path = os.path.join(save_dir, \"SHAP_summary_plot_fish12.20.png\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, Xtest_scaled, feature_names=X.columns, show=False)\n",
    "plt.xlabel('Feature Importance', fontsize=14, weight='bold')\n",
    "plt.ylabel('Features', fontsize=14, weight='bold')\n",
    "plt.title('SHAP Summary Plot', fontsize=16, weight='bold')\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "plt.savefig(summary_plot_path, format=\"png\", bbox_inches=\"tight\", dpi=600)\n",
    "plt.close()\n",
    "bar_plot_path = os.path.join(save_dir, \"SHAP_bar_plot_fish12.20.png\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, Xtest_scaled, plot_type=\"bar\", feature_names=X.columns, show=False)\n",
    "plt.xlabel('Feature Importance', fontsize=14, weight='bold')\n",
    "plt.ylabel('Features', fontsize=14, weight='bold')\n",
    "plt.title('SHAP Bar Plot', fontsize=16, weight='bold')\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "plt.savefig(bar_plot_path, format=\"png\", bbox_inches=\"tight\", dpi=600)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
